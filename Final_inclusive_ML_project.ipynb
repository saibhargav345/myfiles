{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you use anaconda env below installations should possibly suffice\n",
    "pip install shap\n",
    "pip install tensorflow\n",
    "pip install python-whois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and feature creation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import socket\n",
    "import ssl\n",
    "import re\n",
    "import ipaddress\n",
    "from math import log\n",
    "from re import compile\n",
    "from urllib.parse import urlparse\n",
    "from socket import gethostbyname\n",
    "from googlesearch import search\n",
    "from requests import get\n",
    "from json import dump\n",
    "from string import ascii_lowercase\n",
    "from numpy import array\n",
    "import requests\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#read CSV file\n",
    "ds = pd.read_csv(\"C:\\\\Users\\\\dollie\\\\Downloads\\\\urldata.csv\")\n",
    "\n",
    "#shuffling the data to ensure uniqueness\n",
    "ds = ds.sample(frac=1, random_state=42)\n",
    "\n",
    "#pick 5k each\n",
    "ds_good = ds[ds['label']=='good'].sample(n=5000, random_state=42)\n",
    "ds_bad = ds[ds['label']=='bad'].sample(n=5000,random_state=42)\n",
    "\n",
    "#concatenate\n",
    "ds_sampled = pd.concat([ds_good,ds_bad])\n",
    "\n",
    "#changing label\n",
    "ds_sampled['label'] = ds_sampled['label'].map({'good':'benign', 'bad':'malicious'})\n",
    "\n",
    "#feature extraction\n",
    "\n",
    "# 1.Check if url is an IP address\n",
    "def is_ip(url):\n",
    "    match = re.search(r'(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5]))', url)  # IPv4\n",
    "    if match:\n",
    "        return 1  # phishing\n",
    "    else:\n",
    "        return 0  # legit\n",
    "\n",
    "ds_sampled['using_ip'] = ds_sampled['url'].apply(is_ip)\n",
    "\n",
    "# 2. Check if url is long\n",
    "def long_url(url):\n",
    "    if len(url) >= 54:\n",
    "        return 1 #suspicious\n",
    "    else:\n",
    "        return 0 #legit\n",
    "ds_sampled['long_url'] = ds_sampled['url'].apply(lambda x: long_url(x))\n",
    "\n",
    "#3.url shortener\n",
    "def shortening_service(url):\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net', url)\n",
    "    if match:\n",
    "        return 1 #phishing\n",
    "    else:\n",
    "        return 0 #legit\n",
    "ds_sampled['url'] = ds_sampled['url'].astype(str)\n",
    "ds_sampled['using_ip'] = ds_sampled['url'].apply(lambda x: is_ip(x))\n",
    "\n",
    "# 4. Check if url has @ symbol\n",
    "def have_at(url):\n",
    "    if \"@\" in url:\n",
    "        return 1 #phishing\n",
    "    else:\n",
    "        return 0 #legit\n",
    "ds_sampled['having_@_symbol'] = ds_sampled['url'].apply(lambda x: have_at(x))\n",
    "\n",
    "# 5. Check if url uses redirection \"//\"\n",
    "def redirection(url):\n",
    "    position = urlparse(url).path.find(\"//\")\n",
    "    if position > 7:\n",
    "        return 1 #phishing\n",
    "    else:\n",
    "        return 0 #legit\n",
    "ds_sampled['redirection_//_symbol'] = ds_sampled['url'].apply(lambda x: redirection(x))\n",
    "\n",
    "# 6.Adding Prefix or Suffix Separated by (-) to the Domain\n",
    "\n",
    "def has_dash(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    if '-' in domain:\n",
    "        return 1  # Phishing\n",
    "    else:\n",
    "        return 0  # Legit\n",
    "ds_sampled['has_dash_-_symbol'] = ds_sampled['url'].apply(lambda x: has_dash(x))\n",
    "\n",
    "# 7.Sub Domain and Multi Sub Domains\n",
    "\n",
    "def sub_domains(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    if domain.count('.') == 1:\n",
    "        return 0 # legit\n",
    "    else:\n",
    "        return 1 #phish\n",
    "ds_sampled['numof_sub_domains']  = ds_sampled['url'].apply(lambda x: sub_domains(x))  \n",
    "\n",
    "#8. Using Non-Standard Port\n",
    "def non_standard_port(url):\n",
    "    match = re.search(r':\\d+', url)  # port pattern\n",
    "    if match:\n",
    "        port = match.group()[1:]  # Extract the port number without the leading ':'\n",
    "        try:\n",
    "            port = int(port)\n",
    "            if port != 80 and port != 443:  # Not a common port for HTTP or HTTPS\n",
    "                return 1  # phishing\n",
    "        except ValueError:\n",
    "            return 1  # phishing\n",
    "    return 0  # legit\n",
    "\n",
    "ds_sampled['non_std_port'] = ds_sampled['url'].apply(non_standard_port)\n",
    "\n",
    "#9.The Existence of “HTTPS” Token in the Domain Part of the URL\n",
    "\n",
    "def has_https_token(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    if 'https' in domain:\n",
    "        return 1  # Phishing\n",
    "    else:\n",
    "        return 0  # Legit\n",
    "# 10. hostname as substring\n",
    "def abnormal_url(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    hostname = str(hostname)\n",
    "    match = re.search(hostname, url)\n",
    "    if match:\n",
    "        # print match.group()\n",
    "        return 1 #abnormal\n",
    "    else:\n",
    "        # print 'No matching pattern found'\n",
    "        return 0 #normal\n",
    "ds_sampled['abnormal_url'] = ds_sampled['url'].apply(lambda x: abnormal_url(x))\n",
    "\n",
    "# 11. is google indexed?\n",
    "def google_index(url):\n",
    "    site = search(url, 5)\n",
    "    return 1 if site else 0\n",
    "ds_sampled['google_index'] = ds_sampled['url'].apply(lambda x: google_index(x))\n",
    "\n",
    "# 12. number of dots\n",
    "def count_dot(url):\n",
    "    count_dot = url.count('.')\n",
    "    return count_dot\n",
    "ds_sampled['count.'] = ds_sampled['url'].apply(lambda x: count_dot(x))\n",
    "\n",
    "# 13. www count\n",
    "def count_www(url):\n",
    "    url.count('www')\n",
    "    return url.count('www')\n",
    "ds_sampled['count-www'] = ds_sampled['url'].apply(lambda x: count_www(x))\n",
    "\n",
    "# 14. @count\n",
    "def count_atrate(url):\n",
    "     \n",
    "    return url.count('@')\n",
    "ds_sampled['count@'] = ds_sampled['url'].apply(lambda x: count_atrate(x))\n",
    "\n",
    "# 15. no/of dir\n",
    "\n",
    "def no_of_dir(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('/')\n",
    "ds_sampled['count_dir'] = ds_sampled['url'].apply(lambda x: no_of_dir(x))\n",
    "\n",
    "# 16. no.of substrings\n",
    "def no_of_embed(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('//')\n",
    "ds_sampled['count_embed_domian'] = ds_sampled['url'].apply(lambda x: no_of_embed(x))\n",
    "\n",
    "# 17. count%\n",
    "def count_per(url):\n",
    "    return url.count('%')\n",
    "ds_sampled['count%'] = ds_sampled['url'].apply(lambda x : count_per(x))\n",
    "\n",
    "# 18. count ?\n",
    "def count_ques(url):\n",
    "    return url.count('?')\n",
    "ds_sampled['count?'] = ds_sampled['url'].apply(lambda x: count_ques(x))\n",
    "\n",
    "# 19. count -\n",
    "def count_hyphen(url):\n",
    "    return url.count('-')\n",
    "ds_sampled['count-'] = ds_sampled['url'].apply(lambda x: count_hyphen(x))\n",
    "\n",
    "# 20. count =\n",
    "def count_equal(url):\n",
    "    return url.count('=')\n",
    "ds_sampled['count='] = ds_sampled['url'].apply(lambda x: count_equal(x))\n",
    "\n",
    "# 21. url length\n",
    "def url_length(url):\n",
    "    return len(str(url))\n",
    "ds_sampled['url_length'] = ds_sampled['url'].apply(lambda x: url_length(x))\n",
    "\n",
    "# 22. hostname length\n",
    "def hostname_length(url):\n",
    "    return len(urlparse(url).netloc)\n",
    "ds_sampled['hostname_length'] = ds_sampled['url'].apply(lambda x: hostname_length(x))\n",
    "#23\n",
    "ds_sampled.head()\n",
    "#24.sus words\n",
    "\n",
    "def suspicious_words(url):\n",
    "    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',\n",
    "                      url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "ds_sampled['sus_url'] = ds_sampled['url'].apply(lambda x: suspicious_words(x))\n",
    "\n",
    "#25.http\n",
    "def http_over_https(url):\n",
    "    if 'http://' in url:\n",
    "        return 1\n",
    "    return 0\n",
    "ds_sampled['has_http'] = ds_sampled['url'].apply(lambda x: http_over_https(x))\n",
    "\n",
    "#26. special chars\n",
    "def special_chars_in_url(url):\n",
    "    special_chars = [\"<\", \">\", \"#\", \"&\", \"*\"]  \n",
    "    for char in special_chars:\n",
    "        if char in url:\n",
    "            return 1\n",
    "    return 0\n",
    "ds_sampled['has_special_char'] = ds_sampled['url'].apply(lambda x: special_chars_in_url(x))\n",
    "\n",
    "def getDomain(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    if not domain:  # This checks if domain is None or empty\n",
    "        return \"\"   # Returns an empty string instead of None\n",
    "    if re.match(r\"^www.\",domain):\n",
    "        domain = domain.replace(\"www.\",\"\")\n",
    "    return domain\n",
    "\n",
    "# 27. misspell\n",
    "def misspelled_brand_name(url):\n",
    "    domain = getDomain(url)\n",
    "    if domain is not None:\n",
    "        brand_names = [\"goggle\",\"gooqle\",\"googledocument\", \"facebok\",\"face-book\", \"appl\", \"microsotf\",\"micro-soft\",\"microsoftsup\", \"amazn\"]  \n",
    "        for brand in brand_names:\n",
    "            if brand in domain:\n",
    "                return 1\n",
    "    return 0\n",
    "ds_sampled['misspell_brand_name'] = ds_sampled['url'].apply(lambda x: misspelled_brand_name(x))\n",
    "\n",
    "#28. URL entropy\n",
    "def get_entropy(url):\n",
    "    s = url.strip()\n",
    "    prob = [float(s.count(c)) / len(s) for c in dict.fromkeys(list(s))]\n",
    "    entropy = - sum(p * np.log2(p) for p in prob)\n",
    "    return entropy\n",
    "\n",
    "ds_sampled['url_entropy'] = ds_sampled['url'].apply(lambda x: get_entropy(x))\n",
    "\n",
    "#29. Total number of digits in URL string\n",
    "def count_digits(url):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "ds_sampled['count_digits'] = ds_sampled['url'].apply(lambda x: count_digits(x))\n",
    "\n",
    "# 30. Total number of query parameters in URL\n",
    "def count_parameters(url):\n",
    "    parameters = urlparse(url).query.split('&')\n",
    "    if parameters[0] == '':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(parameters)\n",
    "\n",
    "ds_sampled['count_parameters'] = ds_sampled['url'].apply(lambda x: count_parameters(x))\n",
    "\n",
    "# 31.Total number of fragments in URL\n",
    "def count_fragments(url):\n",
    "    fragments = urlparse(url).fragment.split('#')\n",
    "    if fragments[0] == '':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(fragments)\n",
    "\n",
    "ds_sampled['count_fragments'] = ds_sampled['url'].apply(lambda x: count_fragments(x))\n",
    "\n",
    "# 32. Check if page is online\n",
    "def is_online(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        return 1 if response.status_code == 200 else 0\n",
    "    except requests.RequestException:\n",
    "        return 0\n",
    "\n",
    "ds_sampled['is_online'] = ds_sampled['url'].apply(lambda x: is_online(x))\n",
    "\n",
    "# 33. Number of days since domain was registered\n",
    "def days_since_domain_registered(url):\n",
    "    domain_name = urlparse(url).netloc\n",
    "    try:\n",
    "        whois = get_whois(domain_name)\n",
    "        creation_date = whois['creation_date'][0]\n",
    "        if isinstance(creation_date, str):\n",
    "            creation_date = datetime.datetime.strptime(creation_date, \"%Y-%m-%d\")\n",
    "        delta = datetime.datetime.now() - creation_date\n",
    "        return delta.days\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "ds_sampled['days_since_domain_registered'] = ds_sampled['url'].apply(lambda x: days_since_domain_registered(x))\n",
    "\n",
    "#34 Number of days until domain expires\n",
    "def days_until_domain_expires(url):\n",
    "    domain_name = urlparse(url).netloc\n",
    "    try:\n",
    "        whois = get_whois(domain_name)\n",
    "        expiration_date = whois['expiration_date'][0]\n",
    "        if isinstance(expiration_date, str):\n",
    "            expiration_date = datetime.datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "        delta = expiration_date - datetime.datetime.now()\n",
    "        return delta.days\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "ds_sampled['days_until_domain_expires'] = ds_sampled['url'].apply(lambda x: days_until_domain_expires(x))\n",
    "\n",
    "#35 Total number of characters in HTML page\n",
    "def num_html_chars(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        return len(response.content)\n",
    "    except requests.RequestException:\n",
    "        return -1\n",
    "\n",
    "ds_sampled['num_html_chars'] = ds_sampled['url'].apply(lambda x: num_html_chars(x))\n",
    "\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "# 36. Total number of h1-h6 tags in HTML\n",
    "def num_headings(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    return len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))\n",
    "\n",
    "ds_sampled['num_headings'] = ds_sampled['url'].apply(lambda x: num_headings(x))\n",
    "\n",
    "#37. Total number of images in HTML\n",
    "def num_images(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    return len(soup.find_all('img'))\n",
    "\n",
    "ds_sampled['num_images'] = ds_sampled['url'].apply(lambda x: num_images(x))\n",
    "\n",
    "# 38. Total number of links in HTML\n",
    "def num_links(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    return len(soup.find_all('a'))\n",
    "\n",
    "ds_sampled['num_links'] = ds_sampled['url'].apply(lambda x: num_links(x))\n",
    "\n",
    "# 39. Total number of characters in scripts\n",
    "def num_script_chars(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    scripts = soup.find_all('script')\n",
    "    return sum(len(script.text) for script in scripts)\n",
    "\n",
    "ds_sampled['num_script_chars'] = ds_sampled['url'].apply(lambda x: num_script_chars(x))\n",
    "\n",
    "# 40.Total number of special characters\n",
    "def num_special_chars(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    content = str(soup)\n",
    "    special_chars = '!@#$%^&*()_+=-[]{}|;:,.<>?/~`'\n",
    "    return sum(content.count(char) for char in special_chars)\n",
    "\n",
    "ds_sampled['num_special_chars'] = ds_sampled['url'].apply(lambda x: num_special_chars(x))\n",
    "\n",
    "# 41. Script to special character ratio\n",
    "ds_sampled['script_to_special_char_ratio'] = ds_sampled['num_script_chars'] / ds_sampled['num_special_chars']\n",
    "\n",
    "#42.Script to body ratio\n",
    "def script_to_body_ratio(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    body = soup.find('body')\n",
    "    if not body:\n",
    "        return -1\n",
    "    scripts = soup.find_all('script')\n",
    "    return sum(len(script.text) for script in scripts) / len(body.text)\n",
    "\n",
    "ds_sampled['script_to_body_ratio'] = ds_sampled['url'].apply(lambda x: script_to_body_ratio(x))\n",
    "\n",
    "#43. Number of redirects before reaching the final page\n",
    "def count_redirects(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if len(response.history) > 0:\n",
    "            return len(response.history)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "ds_sampled['count_redirects'] = ds_sampled['url'].apply(lambda x: count_redirects(x))\n",
    "\n",
    "#44. Number of links to external domains in the HTML.\n",
    "def count_external_links(html):\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    links = bs.findAll('a', href=re.compile('^http[s]?://'))\n",
    "    return len(links)\n",
    "ds_sampled['count_external_links'] = ds_sampled['url'].apply(lambda x: count_external_links(x))\n",
    "\n",
    "#45.Number of iframe tags in the HTML.\n",
    "def count_iframes(html):\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    iframes = bs.findAll('iframe')\n",
    "    return len(iframes)\n",
    "ds_sampled['count_iframes'] = ds_sampled['url'].apply(lambda x: count_iframes(x))\n",
    "\n",
    "#46.has_privacy_policy - Binary feature indicating whether the website has a privacy policy page.\n",
    "def has_privacy_policy(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return any(['privacy' in a['href'] for a in soup.select('a[href]') if a['href']])\n",
    "    except:\n",
    "        return False\n",
    "ds_sampled['has_privacy_policy'] = ds_sampled['url'].apply(lambda x: has_privacy_policy(x))\n",
    "\n",
    "#47. domain age\n",
    "def domain_registered_long_ago(url):\n",
    "    try:\n",
    "        w = whois.whois(url)\n",
    "        if w.creation_date is None:\n",
    "            return -1\n",
    "        elif type(w.creation_date) is list:\n",
    "            creation_date = w.creation_date[0]\n",
    "        else:\n",
    "            creation_date = w.creation_date\n",
    "\n",
    "        if creation_date < datetime.now() - relativedelta(years=5):  # Change the years as per requirement\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception:\n",
    "        return -1\n",
    "ds_sampled['domain_registered_long_ago'] = ds_sampled['url'].apply(lambda x: domain_registered_long_ago(x))\n",
    "\n",
    "#48. domain does not expire until\n",
    "def domain_expiration_far_future(url):\n",
    "    try:\n",
    "        w = whois.whois(url)\n",
    "        if w.expiration_date is None:\n",
    "            return -1\n",
    "        elif type(w.expiration_date) is list:\n",
    "            expiration_date = w.expiration_date[0]\n",
    "        else:\n",
    "            expiration_date = w.expiration_date\n",
    "\n",
    "        if expiration_date > datetime.now() + relativedelta(years=1):  # Change the years as per requirement\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "ds_sampled['domain_expiration_far_future'] = ds_sampled['url'].apply(lambda x: domain_expiration_far_future(x))\n",
    "\n",
    "#49.favicon loaded from external domain?\n",
    "def favicon_external(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    link_tags = soup.find_all('link', rel=lambda x: x and 'icon' in x)\n",
    "    for tag in link_tags:\n",
    "        favicon_url = tag.get('href')\n",
    "        if not favicon_url:\n",
    "            continue\n",
    "        if 'http' in favicon_url and urlparse(favicon_url).netloc != urlparse(url).netloc:\n",
    "            return 1\n",
    "    return 0\n",
    "ds_sampled['favicon_external'] = ds_sampled['url'].apply(lambda x: favicon_external(x))\n",
    "\n",
    "#50.using iframe =phishing \n",
    "def using_iframe(url):\n",
    "    soup = get_html_content(url)\n",
    "    if not soup:\n",
    "        return -1\n",
    "    iframe_tags = soup.find_all('iframe')\n",
    "    return 1 if iframe_tags else 0 \n",
    "\n",
    "print(ds_sampled.head())\n",
    "#processed dataset\n",
    "ds_sampled.to_csv('C:\\\\Users\\\\dollie\\\\Downloads\\\\processed_urldata_50.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4259203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision trees - final code - includes feature importance\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\dollie\\\\Downloads\\\\processed_urldata_new.csv')\n",
    "\n",
    "# Separate the features (X) from the target variable (y)\n",
    "X = df.drop(['label'], axis=1)  \n",
    "y = df['label']\n",
    "\n",
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform all columns of X\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store accuracy scores\n",
    "scores = []\n",
    "\n",
    "# Initialize fold counter\n",
    "fold = 0\n",
    "\n",
    "# Initialize the DataFrame to store feature importances\n",
    "feature_importances = pd.DataFrame(index=X.columns)\n",
    "\n",
    "# Initialize confusion matrix to store summed confusion matrix\n",
    "cm_sum = np.zeros((2,2))\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to predict the labels of the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and append to scores list\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"\\nFold: {fold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Sum up the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_sum += cm\n",
    "\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save the feature importances\n",
    "    feature_importances[f'fold_{fold}'] = model.feature_importances_\n",
    "\n",
    "# Print the summed confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm_sum, annot=True, fmt=\".1f\")\n",
    "plt.title('Summed confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "# Print the feature importance DataFrame\n",
    "print(\"\\nFeature Importances:\\n\", feature_importances)\n",
    "\n",
    "# Calculate mean and standard deviation of feature importances\n",
    "feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "feature_importances['std'] = feature_importances.std(axis=1)\n",
    "\n",
    "# Sort features by mean importance\n",
    "feature_importances = feature_importances.sort_values(by='mean', ascending=False)\n",
    "\n",
    "# Plot the mean feature importance\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Feature Importances')\n",
    "sns.barplot(x=feature_importances['mean'], y=feature_importances.index, xerr=feature_importances['std'], color='b')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Average cross-validation score: \", sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0abf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM - final code - includes feature importance\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\dollie\\\\Downloads\\\\processed_urldata_new.csv')\n",
    "\n",
    "# Separate the features (X) from the target variable (y)\n",
    "X = df.drop(['label'], axis=1)  \n",
    "y = df['label']\n",
    "\n",
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform all columns of X\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Initialize the Support Vector Machine model with a linear kernel\n",
    "model = SVC(kernel='linear') # Make sure it's linear!\n",
    "\n",
    "# Initialize the DataFrame to store feature importances\n",
    "feature_importances = pd.DataFrame(index=X.columns)\n",
    "\n",
    "# 10-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "# Initialize the cumulative confusion matrix\n",
    "cumulative_cm = np.zeros((2,2)) # Modify this if you have more than two classes\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    fold += 1\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and append to scores list\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"\\nFold: {fold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Add the confusion matrix to the cumulative one\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cumulative_cm += cm\n",
    "\n",
    "    # Save the feature importances\n",
    "    feature_importances[f'fold_{fold}'] = abs(model.coef_[0]) # Absolute value of coefficients\n",
    "\n",
    "# Plot the cumulative confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cumulative_cm, annot=True, fmt=\".1f\")\n",
    "plt.title('Cumulative Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation of feature importances\n",
    "feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "feature_importances['std'] = feature_importances.std(axis=1)\n",
    "\n",
    "# Sort features by mean importance\n",
    "feature_importances = feature_importances.sort_values(by='mean', ascending=False)\n",
    "\n",
    "# Plot the mean feature importance\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Feature Importances')\n",
    "sns.barplot(x=feature_importances['mean'], y=feature_importances.index, xerr=feature_importances['std'], color='b')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "    \n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Average cross-validation score: \", sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187de1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0570a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural networks - includes feature importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap  \n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\dollie\\\\Downloads\\\\processed_urldata_new.csv')\n",
    "\n",
    "# Separate the features (X) from the target variable (y)\n",
    "X = df.drop(['label'], axis=1)  \n",
    "y = df['label']\n",
    "\n",
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform all columns of X\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(le.fit_transform(y))\n",
    "\n",
    "# Initialize the StratifiedKFold class\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store accuracy scores\n",
    "scores = []\n",
    "\n",
    "# Initialize fold counter\n",
    "fold = 0\n",
    "\n",
    "# Initialize the cumulative confusion matrix\n",
    "cumulative_cm = np.zeros((2,2)) # Modify this if you have more than two classes\n",
    "\n",
    "# Convert all data into float data type\n",
    "X = X.astype(float)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y.argmax(1)):\n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))  # 2 represents number of classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Use the trained model to predict the labels of the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate accuracy and append to scores list\n",
    "    accuracy = accuracy_score(y_test.argmax(1), y_pred)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"\\nFold: {fold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Add the confusion matrix to the cumulative one\n",
    "    cm = confusion_matrix(y_test.argmax(1), y_pred)\n",
    "    cumulative_cm += cm\n",
    "\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Average cross-validation score: \", sum(scores)/len(scores))\n",
    "\n",
    "# Plot the cumulative confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cumulative_cm, annot=True, fmt=\".1f\")\n",
    "plt.title('Cumulative Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate SHAP values and plot the summary\n",
    "background = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
    "explainer = shap.KernelExplainer(model.predict, background)\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
